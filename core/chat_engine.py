import asyncio
import time
import json
from typing import List, Dict, Any, Optional, AsyncGenerator, Union
from openai import OpenAI
from config.config import get_config
from utils.log import log
from core.chat_memory import ChatMemory, get_async_chat_memory
from core.personality_manager import PersonalityManager
from services.tools.manager import ToolManager
import httpx
from services.tools.registry import tool_registry
from services.mcp.manager import get_mcp_manager
from services.mcp.exceptions import MCPServiceError, MCPServerNotFoundError, MCPToolNotFoundError
# æ–°å¢æ¨¡å—å¯¼å…¥
from core.openai_client import AsyncOpenAIWrapper
from core.request_builder import build_request_params
from core.tools_adapter import normalize_tool_calls, build_tool_response_messages
from core.token_budget import should_include_memory
from core.prompt_builder import compose_system_prompt
# æ€§èƒ½ç›‘æ§
from utils.performance import performance_monitor, PerformanceMetrics
import uuid
# åŸºç±»å¯¼å…¥
from core.base_engine import BaseEngine, EngineCapabilities, EngineStatus


config = get_config()

class ChatEngine(BaseEngine):
    def __init__(self):
        # åˆå§‹åŒ–åŒæ­¥å®¢æˆ·ç«¯
        self.sync_client = OpenAI(
            api_key=config.OPENAI_API_KEY,
            base_url=config.OPENAI_BASE_URL,
            timeout=config.OPENAI_API_TIMEOUT,
            http_client=httpx.Client(
                follow_redirects=True,
                verify=config.VERIFY_SSL,
                http2=True,
                timeout=httpx.Timeout(
                    connect=config.OPENAI_CONNECT_TIMEOUT,
                    read=config.OPENAI_READ_TIMEOUT,
                    write=config.OPENAI_WRITE_TIMEOUT,
                    pool=config.OPENAI_POOL_TIMEOUT
                ),
                limits=httpx.Limits(
                    max_connections=config.MAX_CONNECTIONS,
                    max_keepalive_connections=config.MAX_KEEPALIVE_CONNECTIONS,
                    keepalive_expiry=config.KEEPALIVE_EXPIRY
                )
            )
        )
        # åŒ…è£…ä¸ºå¼‚æ­¥å®¢æˆ·ç«¯
        self.client = AsyncOpenAIWrapper(self.sync_client)
        self.chat_memory = None
        self.async_chat_memory = None
        self.personality_manager = None
        self.tool_manager = None
        self._initialized = False
    
    def _ensure_initialized(self):
        """ç¡®ä¿ç»„ä»¶å·²åˆå§‹åŒ–ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰"""
        if not self._initialized:
            log.info("ChatEngine: å¼€å§‹å»¶è¿Ÿåˆå§‹åŒ–ç»„ä»¶...")
            self.chat_memory = ChatMemory()
            self.async_chat_memory = get_async_chat_memory()
            self.personality_manager = PersonalityManager()
            self.tool_manager = ToolManager()
            self._initialized = True
            log.info("ChatEngine: å»¶è¿Ÿåˆå§‹åŒ–å®Œæˆ")
       
    # åœ¨generate_responseæ–¹æ³•ä¸­æ·»åŠ æ€§èƒ½ç›‘æ§
    async def generate_response(self, messages: List[Dict[str, str]], conversation_id: str = "default", 
                               personality_id: Optional[str] = None, use_tools: Optional[bool] = None, 
                               stream: Optional[bool] = None) -> Any:
        # ç¡®ä¿ç»„ä»¶å·²åˆå§‹åŒ–
        self._ensure_initialized()
        
        # è®°å½•æ€»è¯·æ±‚å¤„ç†å¼€å§‹æ—¶é—´
        total_start_time = time.time()
        
        # åˆ›å»ºæ€§èƒ½æŒ‡æ ‡å¯¹è±¡
        metrics = PerformanceMetrics(
            request_id=str(uuid.uuid4())[:8],
            timestamp=total_start_time,
            stream=stream if stream is not None else config.STREAM_DEFAULT,
            use_tools=use_tools if use_tools is not None else config.USE_TOOLS_DEFAULT,
            personality_id=personality_id
        )
        try:
            # éªŒè¯è¾“å…¥å‚æ•°
            if not messages or not isinstance(messages, list):
                error_msg = "æ¶ˆæ¯åˆ—è¡¨ä¸èƒ½ä¸ºç©ºä¸”å¿…é¡»æ˜¯åˆ—è¡¨ç±»å‹"
                log.error(error_msg)
                if stream:
                    async def error_gen():
                        yield {"role": "assistant", "content": error_msg, "finish_reason": "error", "stream": True}
                    return error_gen()
                return {"role": "assistant", "content": error_msg}
            
            # éªŒè¯æ¯æ¡æ¶ˆæ¯æ ¼å¼
            for idx, msg in enumerate(messages):
                if not isinstance(msg, dict):
                    error_msg = f"æ¶ˆæ¯ #{idx} æ ¼å¼é”™è¯¯ï¼šå¿…é¡»æ˜¯å­—å…¸ç±»å‹"
                    log.error(error_msg)
                    if stream:
                        async def error_gen():
                            yield {"role": "assistant", "content": error_msg, "finish_reason": "error", "stream": True}
                        return error_gen()
                    return {"role": "assistant", "content": error_msg}
                
                # æ£€æŸ¥å¿…éœ€å­—æ®µï¼Œä½†å…è®¸å·¥å…·æ¶ˆæ¯æ ¼å¼
                if "role" not in msg:
                    error_msg = f"æ¶ˆæ¯ #{idx} æ ¼å¼é”™è¯¯ï¼šç¼ºå°‘å¿…éœ€çš„ 'role' å­—æ®µ"
                    log.error(error_msg)
                    if stream:
                        async def error_gen():
                            yield {"role": "assistant", "content": error_msg, "finish_reason": "error", "stream": True}
                        return error_gen()
                    return {"role": "assistant", "content": error_msg}
                
                # å¯¹äºå·¥å…·æ¶ˆæ¯ï¼Œå…è®¸æ²¡æœ‰contentå­—æ®µï¼ˆä½¿ç”¨tool_call_idï¼‰
                if msg["role"] == "tool":
                    if "tool_call_id" not in msg and "content" not in msg:
                        error_msg = f"æ¶ˆæ¯ #{idx} æ ¼å¼é”™è¯¯ï¼šå·¥å…·æ¶ˆæ¯ç¼ºå°‘ 'tool_call_id' æˆ– 'content' å­—æ®µ"
                        log.error(error_msg)
                        if stream:
                            async def error_gen():
                                yield {"role": "assistant", "content": error_msg, "finish_reason": "error", "stream": True}
                            return error_gen()
                        return {"role": "assistant", "content": error_msg}
                elif msg["role"] == "assistant":
                    # assistantæ¶ˆæ¯å¯ä»¥åŒ…å«tool_callsæˆ–contentï¼Œæˆ–ä¸¤è€…éƒ½æœ‰
                    if "content" not in msg and "tool_calls" not in msg:
                        error_msg = f"æ¶ˆæ¯ #{idx} æ ¼å¼é”™è¯¯ï¼šassistantæ¶ˆæ¯ç¼ºå°‘ 'content' æˆ– 'tool_calls' å­—æ®µ"
                        log.error(error_msg)
                        if stream:
                            async def error_gen():
                                yield {"role": "assistant", "content": error_msg, "finish_reason": "error", "stream": True}
                            return error_gen()
                        return {"role": "assistant", "content": error_msg}
                else:
                    # å…¶ä»–è§’è‰²æ¶ˆæ¯å¿…é¡»æœ‰contentå­—æ®µ
                    if "content" not in msg:
                        error_msg = f"æ¶ˆæ¯ #{idx} æ ¼å¼é”™è¯¯ï¼šç¼ºå°‘å¿…éœ€çš„ 'content' å­—æ®µ"
                        log.error(error_msg)
                        if stream:
                            async def error_gen():
                                yield {"role": "assistant", "content": error_msg, "finish_reason": "error", "stream": True}
                            return error_gen()
                        return {"role": "assistant", "content": error_msg}
            
            # æ‰“å°ä¼ å…¥çš„å‚æ•°å€¼ï¼Œç”¨äºè°ƒè¯•
            log.debug(f"ä¼ å…¥å‚æ•° - personality_id: {personality_id}, use_tools: {use_tools}, stream: {stream}, type(personality_id): {type(personality_id)}, type(use_tools): {type(use_tools)}, type(stream): {type(stream)}")
            
            # åˆ›å»ºmessagesçš„æ·±æ‹·è´ï¼Œé¿å…ä¿®æ”¹åŸå§‹åˆ—è¡¨
            messages_copy = [msg.copy() for msg in messages]
            # è‹¥å†å²è¿‡é•¿ï¼Œä»…ä¿ç•™æœ€å3æ¡ï¼Œå‡å°‘è¯·æ±‚ä½“ä½“ç§¯
            if len(messages_copy) > 3:
                messages_copy = messages_copy[-3:]
            
            log.debug(f"åŸå§‹æ¶ˆæ¯: {messages_copy}")
            
            # å¦‚æœæ²¡æœ‰æŒ‡å®šäººæ ¼IDï¼Œä½¿ç”¨é»˜è®¤äººæ ¼
            if personality_id is None:
                personality_id = config.DEFAULT_PERSONALITY
                log.info(f"æœªæŒ‡å®šäººæ ¼IDï¼Œä½¿ç”¨é»˜è®¤äººæ ¼: {personality_id}")
            
            # å¦‚æœæ²¡æœ‰æŒ‡å®šuse_toolsï¼Œä½¿ç”¨é»˜è®¤å€¼
            if use_tools is None:
                use_tools = config.USE_TOOLS_DEFAULT
                log.info(f"æœªæŒ‡å®šuse_toolsï¼Œä½¿ç”¨é»˜è®¤å€¼: {use_tools}")
            
            # å¦‚æœæ²¡æœ‰æŒ‡å®šstreamï¼Œä½¿ç”¨é»˜è®¤å€¼
            if stream is None:
                stream = config.STREAM_DEFAULT
                log.info(f"æœªæŒ‡å®šstreamï¼Œä½¿ç”¨é»˜è®¤å€¼: {stream}")
            
            # è·å–è®°å¿†å’Œäººæ ¼ä¿¡æ¯
            memory_section = ""
            personality_system = ""
            
            # ä»è®°å¿†ä¸­æ£€ç´¢ç›¸å…³å†…å®¹
            if config.ENABLE_MEMORY_RETRIEVAL and conversation_id != "default" and messages_copy:
                memory_start = time.time()
                query_text = messages_copy[-1]["content"][:100]  # åªè®°å½•å‰100ä¸ªå­—ç¬¦
                log.debug(f"ğŸ” å¼€å§‹æ£€ç´¢è®°å¿†: conversation_id={conversation_id}, query='{query_text}...'")
                relevant_memories = await self.async_chat_memory.get_relevant_memory(conversation_id, messages_copy[-1]["content"])
                metrics.memory_retrieval_time = time.time() - memory_start
                log.debug(f"ğŸ” è®°å¿†æ£€ç´¢å®Œæˆ: conversation_id={conversation_id}, è€—æ—¶={metrics.memory_retrieval_time:.3f}s, æ‰¾åˆ°{len(relevant_memories)}æ¡è®°å¿†")
                
                # æ£€æŸ¥æ˜¯å¦å‘½ä¸­ç¼“å­˜ï¼ˆé€šè¿‡æ£€ç´¢æ—¶é—´åˆ¤æ–­ï¼‰
                metrics.memory_cache_hit = metrics.memory_retrieval_time < 0.01  # å°äº10msè®¤ä¸ºæ˜¯ç¼“å­˜å‘½ä¸­
                
                if relevant_memories:
                    memory_text = "\n".join(relevant_memories)
                    memory_section = f"å‚è€ƒè®°å¿†ï¼š\n{memory_text}"
                    log.debug(f"âœ… æ£€ç´¢åˆ°ç›¸å…³è®°å¿† {len(relevant_memories)} æ¡ï¼Œå†…å®¹é¢„è§ˆ: {memory_text[:200]}...")
                    
                    # ä½¿ç”¨æ–°çš„tokené¢„ç®—æ¨¡å—æ£€æŸ¥æ˜¯å¦åº”è¯¥åŒ…å«è®°å¿†
                    max_tokens = getattr(config, 'OPENAI_MAX_TOKENS', 8192)
                    if not should_include_memory(messages_copy, memory_section, max_tokens):
                        log.warning("âš ï¸ é¿å…è¶…å‡ºæ¨¡å‹tokené™åˆ¶ï¼Œä¸æ·»åŠ è®°å¿†åˆ°ç³»ç»Ÿæç¤º")
                        memory_section = ""
                    else:
                        log.debug(f"âœ… è®°å¿†å·²æ·»åŠ åˆ°ç³»ç»Ÿæç¤ºï¼ŒåŒ…å« {len(relevant_memories)} æ¡è®°å¿†")
                else:
                    log.debug(f"âš ï¸ æœªæ£€ç´¢åˆ°ç›¸å…³è®°å¿†: conversation_id={conversation_id}")
            elif not config.ENABLE_MEMORY_RETRIEVAL:
                log.debug("âŒ Memoryæ£€ç´¢å·²ç¦ç”¨ (ENABLE_MEMORY_RETRIEVAL=false)")
            elif conversation_id == "default":
                log.debug(f"âš ï¸ Memoryæ£€ç´¢è·³è¿‡: conversation_id='default' (é»˜è®¤ä¼šè¯ä¸ä½¿ç”¨è®°å¿†)")
            elif not messages_copy:
                log.debug(f"âš ï¸ Memoryæ£€ç´¢è·³è¿‡: messages_copyä¸ºç©º")
            
            # è·å–äººæ ¼çš„ç³»ç»Ÿæç¤º
            if personality_id:
                try:
                    personality = self.personality_manager.get_personality(personality_id)
                    if personality:
                        personality_system = personality.system_prompt or ""
                except Exception as e:
                    log.warning(f"è·å–äººæ ¼æ—¶å‡ºé”™ï¼Œå¿½ç•¥äººæ ¼è®¾ç½®: {e}")
            
            # ä½¿ç”¨æ–°çš„æç¤ºæ„å»ºå™¨åˆæˆç³»ç»Ÿæç¤º
            messages_copy = compose_system_prompt(messages_copy, personality_system, memory_section)
            log.debug(f"åˆæˆç³»ç»Ÿæç¤ºåæ¶ˆæ¯: {messages_copy}")
            
            # ä½¿ç”¨æ–°çš„æ–¹æ³•è·å–å…è®¸çš„å·¥å…·schemaï¼ˆå·²æ ¹æ®personalityè¿‡æ»¤ï¼‰
            allowed_tools_schema = await self.get_allowed_tools_schema(personality_id) if use_tools else None
            request_params = build_request_params(
                model=config.OPENAI_MODEL,
                temperature=float(config.OPENAI_TEMPERATURE),
                messages=messages_copy,
                use_tools=use_tools,
                all_tools_schema=allowed_tools_schema,
                allowed_tool_names=None  # ä¸å†éœ€è¦å•ç‹¬ä¼ é€’ï¼Œå·²åœ¨schemaä¸­è¿‡æ»¤
            )
            
            log.debug(f"æœ€ç»ˆè¯·æ±‚å‚æ•°: {request_params}")
            # è®°å½•æ€»è¯·æ±‚å¤„ç†æ—¶é—´
            log.debug(f"æ€»è¯·æ±‚å¤„ç†æ—¶é—´ä¸€: {time.time() - total_start_time:.2f}ç§’")
            if stream:
                # åŒ…è£…å¼‚æ­¥ç”Ÿæˆå™¨ä»¥ç¡®ä¿æ€§èƒ½æŒ‡æ ‡è¢«è®°å½•
                return self._wrap_streaming_response_with_performance(
                    self._generate_streaming_response(request_params, conversation_id, messages, personality_id, metrics),
                    metrics, total_start_time
                )
            else:
                result = await self._generate_non_streaming_response(request_params, conversation_id, messages, personality_id, metrics)
                
                # è®°å½•æ€§èƒ½æŒ‡æ ‡
                metrics.total_time = time.time() - total_start_time
                if config.ENABLE_PERFORMANCE_MONITOR:
                    performance_monitor.record(metrics, log_enabled=config.PERFORMANCE_LOG_ENABLED)
                
                return result
        except Exception as e:
            log.error(f"Error in generate_response: {e}")
            # è¿”å›é€‚å½“çš„é”™è¯¯å“åº”å¯¹è±¡ï¼Œè€Œä¸æ˜¯ç®€å•çš„é”™è¯¯å­—å…¸
            if stream:
                # å¯¹äºæµå¼å“åº”ï¼Œè¿”å›ä¸€ä¸ªå¯ä»¥å¼‚æ­¥è¿­ä»£çš„å¯¹è±¡
                async def error_generator():
                    yield {
                        "role": "assistant",
                        "content": f"å‘ç”Ÿé”™è¯¯: {str(e)}",
                        "finish_reason": "error",
                        "stream": True
                    }
                return error_generator()
            else:
                # å¯¹äºéæµå¼å“åº”ï¼Œè¿”å›æ ‡å‡†çš„é”™è¯¯æ¶ˆæ¯æ ¼å¼
                return {"role": "assistant", "content": f"å‘ç”Ÿé”™è¯¯: {str(e)}"}
    
    async def _generate_non_streaming_response(
        self,
        request_params: Dict[str, Any],
        conversation_id: str,
        original_messages: List[Dict[str, str]],
        personality_id: Optional[str] = None,
        metrics: Optional[PerformanceMetrics] = None
    ) -> Dict[str, Any]:
        try:
            # è°ƒç”¨å¼‚æ­¥OpenAI API
            response = await self.client.create_chat(request_params)
            log.debug(f"OpenAI APIå“åº”: {response}")
            
            # å¢åŠ å“åº”æ ¼å¼éªŒè¯
            if not hasattr(response, 'choices') or not response.choices:
                log.error(f"Invalid API response format: {dir(response)}")
                return {"role": "assistant", "content": "è·å–AIå“åº”æ—¶å‘ç”Ÿæ ¼å¼é”™è¯¯ï¼Œè¯·æ£€æŸ¥APIé…ç½®ã€‚"}
            
            # å¤„ç†å“åº”
            if hasattr(response.choices[0].message, 'tool_calls') and response.choices[0].message.tool_calls:
                # å¤„ç†å·¥å…·è°ƒç”¨
                return await self._handle_tool_calls(
                    response.choices[0].message.tool_calls,
                    conversation_id,
                    original_messages,
                    personality_id
                )
            else:
                # ç¡®ä¿contentå­˜åœ¨
                if not hasattr(response.choices[0].message, 'content') or response.choices[0].message.content is None:
                    log.error(f"Response missing content: {response}")
                    return {"role": "assistant", "content": "AIå“åº”å†…å®¹ä¸ºç©ºï¼Œè¯·æ£€æŸ¥APIé…ç½®ã€‚"}
                
                # æ™®é€šå“åº”
                content = response.choices[0].message.content
                
                # ä¿å­˜åˆ°è®°å¿† - ä½¿ç”¨åŸç”Ÿå¼‚æ­¥API
                if conversation_id:
                    # åˆ›å»ºå¼‚æ­¥ä»»åŠ¡ä½†ä¸ç­‰å¾…å…¶å®Œæˆ
                    asyncio.create_task(self._async_save_message_to_memory(
                        conversation_id, 
                        [{"role": "assistant", "content": content}, original_messages[-1]]
                    ))
                
                return {
                    "role": "assistant",
                    "content": content,
                    "model": response.model if hasattr(response, 'model') else "unknown",
                    "usage": {
                        "prompt_tokens": response.usage.prompt_tokens if hasattr(response, 'usage') and response.usage else 0,
                        "completion_tokens": response.usage.completion_tokens if hasattr(response, 'usage') and response.usage else 0,
                        "total_tokens": response.usage.total_tokens if hasattr(response, 'usage') and response.usage else 0
                    }
                }
                
        except json.JSONDecodeError as e:
            log.error(f"JSONè§£æé”™è¯¯: {e}. è¯·æ£€æŸ¥APIç«¯ç‚¹æ˜¯å¦æ­£ç¡®ä¸”è¿”å›æœ‰æ•ˆJSONæ ¼å¼ã€‚")
            return {"role": "assistant", "content": "APIè¿”å›å†…å®¹æ ¼å¼é”™è¯¯ï¼Œè¯·ç¡®è®¤APIç«¯ç‚¹é…ç½®æ­£ç¡®ã€‚"}
        except Exception as e:
            log.error(f"ç”Ÿæˆå“åº”æ—¶å‡ºé”™: {e}")
            # å°è¯•è·å–æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
            detailed_error = str(e)
            if hasattr(e, 'response') and hasattr(e.response, 'text'):
                detailed_error += f"\nå“åº”å†…å®¹: {e.response.text[:200]}..."
            return {"role": "assistant", "content": f"æŠ±æ­‰ï¼Œæˆ‘ç°åœ¨æ— æ³•ä¸ºæ‚¨æä¾›å¸®åŠ©ã€‚é”™è¯¯ä¿¡æ¯: {detailed_error}"}
    
    async def _wrap_streaming_response_with_performance(
        self, 
        streaming_generator: AsyncGenerator[Dict[str, Any], None], 
        metrics: PerformanceMetrics, 
        total_start_time: float
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """åŒ…è£…æµå¼å“åº”ç”Ÿæˆå™¨ï¼Œç¡®ä¿æ€§èƒ½æŒ‡æ ‡è¢«è®°å½•"""
        log.info(f"[PERF WRAPPER] å¼€å§‹åŒ…è£…æµå¼å“åº”ï¼Œrequest_id={metrics.request_id}")
        first_chunk_time = None
        chunk_count = 0
        
        try:
            async for chunk in streaming_generator:
                chunk_count += 1
                if chunk_count == 1:
                    first_chunk_time = time.time()
                    log.debug(f"æµå¼å“åº”é¦–å­—èŠ‚æ—¶é—´: {first_chunk_time - total_start_time:.2f}ç§’")
                
                #log.debug(f"æµå¼å“åº”chunk {chunk_count}: {chunk}")
                yield chunk
                
        except Exception as e:
            log.error(f"æµå¼å“åº”åŒ…è£…å™¨å‡ºé”™: {e}")
            # é‡æ–°æŠ›å‡ºå¼‚å¸¸
            raise e
        finally:
            # æ— è®ºæ˜¯å¦å‘ç”Ÿå¼‚å¸¸ï¼Œéƒ½è®°å½•æ€§èƒ½æŒ‡æ ‡
            log.info(f"[PERF WRAPPER] æµå¼å“åº”å®Œæˆï¼ˆfinallyå—ï¼‰ï¼Œchunk_count={chunk_count}")
            metrics.total_time = time.time() - total_start_time
            if first_chunk_time:
                metrics.first_chunk_time = first_chunk_time - total_start_time
            metrics.openai_api_time = metrics.total_time  # æµå¼å“åº”ä¸­ï¼Œæ€»æ—¶é—´å°±æ˜¯APIæ—¶é—´
            
            log.info(f"[PERF WRAPPER] å‡†å¤‡è®°å½•æ€§èƒ½æŒ‡æ ‡: ENABLE={config.ENABLE_PERFORMANCE_MONITOR}, total_time={metrics.total_time:.3f}s")
            if config.ENABLE_PERFORMANCE_MONITOR:
                log.info(f"[PERF] è®°å½•æµå¼å“åº”æ€§èƒ½æŒ‡æ ‡: total_time={metrics.total_time:.3f}s, chunks={chunk_count}, stream=True")
                performance_monitor.record(metrics, log_enabled=config.PERFORMANCE_LOG_ENABLED)
                log.info(f"[PERF] æ€§èƒ½æŒ‡æ ‡å·²è®°å½•")
    
    async def _generate_streaming_response(
        self, 
        request_params: Dict[str, Any],
        conversation_id: str,
        original_messages: List[Dict[str, str]],
        personality_id: Optional[str] = None,
        metrics: Optional[PerformanceMetrics] = None
    ) -> AsyncGenerator[Dict[str, Any], None]:
        try:
            # è®°å½•APIè°ƒç”¨å¼€å§‹æ—¶é—´
            api_start_time = time.time()
            
            # åˆå§‹åŒ–å˜é‡
            tool_calls = None
            full_content = ""
            chunk_count = 0
            first_chunk_time = None
            
            # ä½¿ç”¨å¼‚æ­¥æµå¼API
            async for chunk in self.client.create_chat_stream(request_params):
                chunk_count += 1
                if chunk_count == 1:
                    first_chunk_time = time.time()
                    log.debug(f"é¦–å­—èŠ‚å“åº”æ—¶é—´: {first_chunk_time - api_start_time:.2f}ç§’")
                    
                if chunk.choices and len(chunk.choices) > 0:
                    choice = chunk.choices[0]
                    
                    # æ£€æµ‹å·¥å…·è°ƒç”¨
                    if hasattr(choice.delta, 'tool_calls') and choice.delta.tool_calls:
                        if not tool_calls:
                            tool_calls = []
                        # æ”¶é›†å·¥å…·è°ƒç”¨ä¿¡æ¯
                        for tool_call in choice.delta.tool_calls:
                            log.debug(f"æ”¶åˆ°å·¥å…·è°ƒç”¨ chunk: index={tool_call.index}, id={getattr(tool_call, 'id', None)}, "
                                     f"has_function={hasattr(tool_call, 'function')}, "
                                     f"function={getattr(tool_call, 'function', None)}")
                            
                            # åˆå§‹åŒ–æˆ–æ›´æ–°å·¥å…·è°ƒç”¨ä¿¡æ¯
                            if tool_call.index >= len(tool_calls):
                                tool_calls.append({"id": None, "type": "function", "function": {}})
                            
                            # æ›´æ–° IDï¼ˆå¯èƒ½åœ¨åç»­ chunk ä¸­æ‰æä¾›ï¼‰
                            if hasattr(tool_call, 'id') and tool_call.id:
                                tool_calls[tool_call.index]["id"] = tool_call.id
                                log.debug(f"æ›´æ–°å·¥å…·è°ƒç”¨ ID: index={tool_call.index}, id={tool_call.id}")
                            
                            # æ›´æ–°å‡½æ•°åç§°å’Œå‚æ•°
                            if hasattr(tool_call, 'function') and tool_call.function:
                                if hasattr(tool_call.function, 'name') and tool_call.function.name:
                                    tool_calls[tool_call.index]["function"]["name"] = tool_call.function.name
                                    log.debug(f"æ›´æ–°å·¥å…·åç§°: index={tool_call.index}, name={tool_call.function.name}")
                                if hasattr(tool_call.function, 'arguments') and tool_call.function.arguments:
                                    if "arguments" not in tool_calls[tool_call.index]["function"]:
                                        tool_calls[tool_call.index]["function"]["arguments"] = ""
                                    tool_calls[tool_call.index]["function"]["arguments"] += tool_call.function.arguments
                                    log.debug(f"ç´¯åŠ å·¥å…·å‚æ•°: index={tool_call.index}, å½“å‰é•¿åº¦={len(tool_calls[tool_call.index]['function']['arguments'])}")
                    
                    # å¤„ç†æ™®é€šå†…å®¹ï¼Œä¼˜åŒ–åˆ†å—è¾“å‡º
                    elif choice.delta.content is not None:
                        content = choice.delta.content
                        full_content += content
                        
                        # å¯¹äºå¤§å—å†…å®¹ï¼Œå¯ä»¥è€ƒè™‘æŒ‰æ ‡ç‚¹ç¬¦å·æˆ–å›ºå®šé•¿åº¦åˆ†å—
                        if len(content) > 100:  # è¶…è¿‡100å­—ç¬¦çš„å¤§å—å†…å®¹
                            # å¯»æ‰¾åˆé€‚çš„åˆ†å—ç‚¹
                            split_points = [i for i, c in enumerate(content) if c in ['.', '!', '?', 'ï¼Œ', 'ã€‚', 'ï¼', 'ï¼Ÿ']]
                            if split_points:
                                # æœ‰æ ‡ç‚¹ç¬¦å·ï¼ŒæŒ‰æ ‡ç‚¹åˆ†å—
                                last_idx = 0
                                for idx in split_points:
                                    yield {
                                        "role": "assistant",
                                        "content": content[last_idx:idx+1],
                                        "finish_reason": None,
                                        "stream": True
                                    }
                                    last_idx = idx + 1
                                # å‘é€å‰©ä½™éƒ¨åˆ†
                                if last_idx < len(content):
                                    yield {
                                        "role": "assistant",
                                        "content": content[last_idx:],
                                        "finish_reason": None,
                                        "stream": True
                                    }
                            else:
                                # æ²¡æœ‰æ ‡ç‚¹ç¬¦å·ï¼ŒæŒ‰å›ºå®šé•¿åº¦åˆ†å—
                                for i in range(0, len(content), 100):
                                    yield {
                                        "role": "assistant",
                                        "content": content[i:i+100],
                                        "finish_reason": None,
                                        "stream": True
                                    }
                        else:
                            # å°å—å†…å®¹ç›´æ¥å‘é€
                            yield {
                                "role": "assistant",
                                "content": content,
                                "finish_reason": None,
                                "stream": True
                            }
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨éœ€è¦å¤„ç†
            if tool_calls:
                log.debug(f"æ”¶é›†åˆ°çš„å·¥å…·è°ƒç”¨åŸå§‹æ•°æ®: {tool_calls}")
                
                # éªŒè¯æ‰€æœ‰å·¥å…·è°ƒç”¨éƒ½æœ‰æœ‰æ•ˆçš„ ID
                for idx, call in enumerate(tool_calls):
                    if not call.get("id"):
                        # ç”Ÿæˆä¸€ä¸ªä¸´æ—¶ ID
                        call["id"] = f"call_{idx}_{int(time.time() * 1000)}"
                        log.warning(f"å·¥å…·è°ƒç”¨ #{idx} ç¼ºå°‘ IDï¼Œå·²ç”Ÿæˆä¸´æ—¶ ID: {call['id']}")
                
                log.debug(f"éªŒè¯åçš„å·¥å…·è°ƒç”¨æ•°æ®: {tool_calls}")
                
                # ä½¿ç”¨æ–°çš„å·¥å…·é€‚é…å™¨è§„èŒƒåŒ–å·¥å…·è°ƒç”¨
                normalized_calls = normalize_tool_calls(tool_calls)
                log.debug(f"è§„èŒƒåŒ–åçš„å·¥å…·è°ƒç”¨: {normalized_calls}")
                
                # å‡†å¤‡å·¥å…·è°ƒç”¨åˆ—è¡¨
                calls_to_execute = []
                for call in normalized_calls:
                    # å®‰å…¨åœ°è§£æ JSON å‚æ•°
                    args_str = call["function"]["arguments"]
                    try:
                        parameters = json.loads(args_str) if args_str else {}
                    except json.JSONDecodeError as e:
                        log.error(f"å·¥å…·å‚æ•° JSON è§£æå¤±è´¥: {args_str}, é”™è¯¯: {e}")
                        parameters = {}
                    
                    calls_to_execute.append({
                        "name": call["function"]["name"],
                        "parameters": parameters
                    })
                
                # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰å·¥å…·è°ƒç”¨
                tool_results = await self.tool_manager.execute_tools_concurrently(calls_to_execute)
                
                # ä½¿ç”¨æ–°çš„å·¥å…·é€‚é…å™¨æ„å»ºå·¥å…·å“åº”æ¶ˆæ¯
                tool_response_messages = build_tool_response_messages(normalized_calls, tool_results)
                
                # æ„å»ºæ–°çš„æ¶ˆæ¯å†å²ï¼ˆåŒ…å«å·¥å…·è°ƒç”¨å’Œç»“æœï¼‰
                new_messages = original_messages.copy()
                assistant_message_with_tools = {"role": "assistant", "tool_calls": normalized_calls}
                log.debug(f"å‡†å¤‡å‘é€çš„ assistant æ¶ˆæ¯: {assistant_message_with_tools}")
                new_messages.append(assistant_message_with_tools)
                new_messages.extend(tool_response_messages)
                log.debug(f"å®Œæ•´çš„æ–°æ¶ˆæ¯å†å²ï¼ˆå…±{len(new_messages)}æ¡ï¼‰: {json.dumps(new_messages, ensure_ascii=False, indent=2)}")
                
                # è·å– personality ç³»ç»Ÿæç¤ºå¹¶æ·»åŠ åˆ°æ¶ˆæ¯ä¸­
                personality_system_prompt = ""
                if personality_id:
                    try:
                        personality = self.personality_manager.get_personality(personality_id)
                        if personality:
                            personality_system_prompt = personality.system_prompt or ""
                    except Exception as e:
                        log.warning(f"è·å–äººæ ¼æ—¶å‡ºé”™: {e}")
                
                # ä½¿ç”¨ compose_system_prompt æ·»åŠ ç³»ç»Ÿæç¤º
                new_messages_with_system = compose_system_prompt(new_messages, personality_system_prompt, "")
                log.debug(f"æ·»åŠ ç³»ç»Ÿæç¤ºåçš„æ¶ˆæ¯ï¼ˆå…±{len(new_messages_with_system)}æ¡ï¼‰")
                
                # é‡æ–°æ„å»ºè¯·æ±‚å‚æ•°ï¼Œç»§ç»­æµå¼ç”Ÿæˆ
                follow_up_params = build_request_params(
                    model=config.OPENAI_MODEL,
                    temperature=float(config.OPENAI_TEMPERATURE),
                    messages=new_messages_with_system,  # ä½¿ç”¨åŒ…å«ç³»ç»Ÿæç¤ºçš„æ¶ˆæ¯
                    use_tools=False,  # å·¥å…·è°ƒç”¨åä¸å†ä½¿ç”¨å·¥å…·
                    all_tools_schema=None,
                    allowed_tool_names=None,
                    force_tool_from_message=False  # ä¸å¼ºåˆ¶é€‰æ‹©å·¥å…·
                )
                
                # ç»§ç»­æµå¼è¾“å‡ºå·¥å…·è°ƒç”¨åçš„å›ç­”
                follow_up_content = ""
                async for follow_up_chunk in self.client.create_chat_stream(follow_up_params):
                    if follow_up_chunk.choices and len(follow_up_chunk.choices) > 0:
                        choice = follow_up_chunk.choices[0]
                        if choice.delta.content is not None:
                            follow_up_content += choice.delta.content
                            yield {
                                "role": "assistant",
                                "content": choice.delta.content,
                                "finish_reason": None,
                                "stream": True
                            }
                
                # ä¿å­˜å·¥å…·è°ƒç”¨åçš„å“åº”åˆ°è®°å¿†
                if conversation_id and follow_up_content:
                    asyncio.create_task(self._async_save_message_to_memory(
                        conversation_id, 
                        [{"role": "assistant", "content": follow_up_content}, original_messages[-1]]
                    ))
                
                # å‘é€ç»“æŸæ ‡å¿—
                    yield {
                        "role": "assistant",
                    "content": "",
                        "finish_reason": "stop",
                        "stream": True
                    }
            else:
                # ä¿å­˜åˆ°è®°å¿† - ä½¿ç”¨åŸç”Ÿå¼‚æ­¥API
                if conversation_id and full_content:
                    # åˆ›å»ºå¼‚æ­¥ä»»åŠ¡ä½†ä¸ç­‰å¾…å…¶å®Œæˆ
                    asyncio.create_task(self._async_save_message_to_memory(
                        conversation_id, 
                        [{"role": "assistant", "content": full_content}, original_messages[-1]]
                    ))
                
                # å‘é€ç»“æŸæ ‡å¿—
                yield {
                    "role": "assistant",
                    "content": "",
                    "finish_reason": "stop",
                    "stream": True
                }
            log.debug(f"æ€»å—æ•°: {chunk_count}, æ€»å¤„ç†æ—¶é—´: {time.time() - api_start_time:.2f}ç§’")
        except Exception as e:
            log.error(f"Error generating streaming response: {e}")
            yield {
                "role": "assistant",
                "content": f"æŠ±æ­‰ï¼Œæˆ‘ç°åœ¨æ— æ³•ä¸ºæ‚¨æä¾›å¸®åŠ©ã€‚é”™è¯¯ä¿¡æ¯: {str(e)}",
                "finish_reason": "error",
                "stream": True
            }
    
    async def _handle_tool_calls(
        self,
        tool_calls: list,
        conversation_id: str,
        original_messages: List[Dict[str, str]],
        personality_id: Optional[str] = None
    ) -> Dict[str, Any]:
        # ä½¿ç”¨æ–°çš„å·¥å…·é€‚é…å™¨è§„èŒƒåŒ–å·¥å…·è°ƒç”¨
        normalized_calls = normalize_tool_calls(tool_calls)
        
        # å‡†å¤‡å·¥å…·è°ƒç”¨åˆ—è¡¨
        calls_to_execute = []
        for call in normalized_calls:
            # å®‰å…¨åœ°è§£æ JSON å‚æ•°
            args_str = call["function"]["arguments"]
            try:
                parameters = json.loads(args_str) if args_str else {}
            except json.JSONDecodeError as e:
                log.error(f"å·¥å…·å‚æ•° JSON è§£æå¤±è´¥: {args_str}, é”™è¯¯: {e}")
                parameters = {}
            
            calls_to_execute.append({
                "name": call["function"]["name"],
                "parameters": parameters
            })
        
        # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰å·¥å…·è°ƒç”¨
        tool_results = await self.tool_manager.execute_tools_concurrently(calls_to_execute)
        
        # ä½¿ç”¨æ–°çš„å·¥å…·é€‚é…å™¨æ„å»ºå·¥å…·å“åº”æ¶ˆæ¯
        tool_response_messages = build_tool_response_messages(normalized_calls, tool_results)
        
        # å°†å·¥å…·è°ƒç”¨ç»“æœæ·»åŠ åˆ°æ¶ˆæ¯å†å²ä¸­ï¼Œå†æ¬¡è°ƒç”¨æ¨¡å‹
        new_messages = original_messages.copy()
        new_messages.append({"role": "assistant", "tool_calls": normalized_calls})
        new_messages.extend(tool_response_messages)
        
        # é‡æ–°ç”Ÿæˆå“åº” - æ˜ç¡®è®¾ç½®stream=Falseï¼Œä¼ é€’personality_idé¿å…é‡å¤åº”ç”¨
        return await self.generate_response(new_messages, conversation_id, personality_id=personality_id, use_tools=False, stream=False)
    
    def call_mcp_service(self, tool_name: str = None, params: dict = None, 
                         service_name: str = None, method_name: str = None, 
                         mcp_server: str = None):
        """è°ƒç”¨MCPæœåŠ¡"""
        try:
            # å‚æ•°éªŒè¯
            params = params or {}
            
            # ç¡®å®šå·¥å…·åç§°
            if not tool_name:
                if not service_name or not method_name:
                    raise ValueError("Either 'tool_name' or both 'service_name' and 'method_name' must be provided")
                tool_name = f"{service_name}__{method_name}"
            
            log.info(f"Calling MCP service: {tool_name}, params: {params}, server: {mcp_server or 'auto'}")
            
            # ä½¿ç”¨MCPç®¡ç†å™¨è°ƒç”¨æœåŠ¡ï¼Œæ”¯æŒæŒ‡å®šæœåŠ¡å™¨
            result = get_mcp_manager().call_tool(tool_name, params, mcp_server)
            
            # æ›´çµæ´»çš„ç»“æœå¤„ç†
            processed_result = []
            if isinstance(result, list):
                for item in result:
                    if isinstance(item, dict) and 'text' in item:
                        processed_result.append(item['text'])
                    else:
                        processed_result.append(str(item))
                response_text = "\n".join(processed_result) if processed_result else str(result)
            else:
                response_text = str(result)
            
            return {"success": True, "result": response_text, "raw_result": result}
        except MCPServerNotFoundError as e:
            log.error(f"MCP server error: {str(e)}")
            return {"success": False, "error": f"MCPæœåŠ¡å™¨é”™è¯¯: {str(e)}"}
        except MCPToolNotFoundError as e:
            log.error(f"MCP tool error: {str(e)}")
            return {"success": False, "error": f"MCPå·¥å…·é”™è¯¯: {str(e)}"}
        except MCPServiceError as e:
            log.error(f"MCP service error: {str(e)}")
            return {"success": False, "error": f"MCPæœåŠ¡è°ƒç”¨å¤±è´¥: {str(e)}"}
        except Exception as e:
            log.error(f"Unexpected error when calling MCP service: {str(e)}")
            return {"success": False, "error": f"è°ƒç”¨MCPæœåŠ¡æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {str(e)}"}

    # æ¸…é™¤ä¼šè¯è®°å¿†
    def clear_conversation_memory(self, conversation_id: str):
        self.chat_memory.delete_memory(conversation_id)
    
    # è·å–ä¼šè¯è®°å¿† - åŒæ­¥ç‰ˆæœ¬
    def get_conversation_memory(self, conversation_id: str = "default") -> list:
        """è·å–å¯¹è¯çš„æ‰€æœ‰è®°å¿†ï¼Œä½¿ç”¨åŒæ­¥æ–¹æ³•ç›´æ¥è·å–"""
        try:
            # ç›´æ¥ä½¿ç”¨åŒæ­¥ç‰ˆæœ¬çš„chat_memoryå®ä¾‹è·å–è®°å¿†
            memories = self.chat_memory.get_all_memory(conversation_id)
            log.debug(f"Successfully retrieved {len(memories)} memories for conversation {conversation_id}")
            return memories
        except Exception as e:
            log.error(f"Failed to get conversation memory: {e}")
            return []

    # è·å–ä¼šè¯è®°å¿† - å¼‚æ­¥ç‰ˆæœ¬
    async def aget_conversation_memory(self, conversation_id: str) -> list:
        """è·å–ä¼šè¯è®°å¿†ï¼ˆå¼‚æ­¥ç‰ˆæœ¬ï¼‰"""
        try:
            # ç›´æ¥è°ƒç”¨å¼‚æ­¥è®°å¿†å®ä¾‹çš„æ–¹æ³•
            return await self.async_chat_memory.get_all_memory(conversation_id)
        except Exception as e:
            log.error(f"å¼‚æ­¥è·å–ä¼šè¯è®°å¿†æ—¶å‡ºé”™: {e}")
            return []

    # å¼‚æ­¥ä¿å­˜æ¶ˆæ¯åˆ°è®°å¿† - ä½¿ç”¨åŸç”Ÿå¼‚æ­¥API
    async def _async_save_message_to_memory(self, conversation_id: str, messages: list):
        """å¼‚æ­¥ä¿å­˜æ¶ˆæ¯åˆ°è®°å¿†ï¼Œä½¿ç”¨mem0çš„åŸç”ŸAsyncMemory API"""
        try:
            log.debug(f"ğŸ’¾ å¼€å§‹ä¿å­˜æ¶ˆæ¯åˆ°è®°å¿†: conversation_id={conversation_id}, æ€»æ¶ˆæ¯æ•°={len(messages)}, MEMORY_SAVE_MODE={config.MEMORY_SAVE_MODE}")
            
            # æ ¹æ®é…ç½®å†³å®šä¿å­˜å“ªäº›æ¶ˆæ¯
            messages_to_save = []
            
            if config.MEMORY_SAVE_MODE == "both":
                # ä¿å­˜æ‰€æœ‰æ¶ˆæ¯ï¼ˆåŠ©æ‰‹å›å¤å’Œç”¨æˆ·è¾“å…¥ï¼‰
                messages_to_save = messages
            elif config.MEMORY_SAVE_MODE == "user_only":
                # åªä¿å­˜ç”¨æˆ·è¾“å…¥çš„æ¶ˆæ¯
                messages_to_save = [msg for msg in messages if msg.get("role") == "user"]
            elif config.MEMORY_SAVE_MODE == "assistant_only":
                # åªä¿å­˜åŠ©æ‰‹å›å¤çš„æ¶ˆæ¯
                messages_to_save = [msg for msg in messages if msg.get("role") == "assistant"]
            else:
                # é»˜è®¤ä¿å­˜æ‰€æœ‰æ¶ˆæ¯
                messages_to_save = messages
                log.warning(f"æœªçŸ¥çš„MEMORY_SAVE_MODEé…ç½®å€¼: {config.MEMORY_SAVE_MODE}ï¼Œé»˜è®¤ä¿å­˜æ‰€æœ‰æ¶ˆæ¯")
            
            if messages_to_save:
                roles = [msg.get("role") for msg in messages_to_save]
                log.debug(f"ğŸ’¾ å‡†å¤‡ä¿å­˜æ¶ˆæ¯åˆ°è®°å¿†: conversation_id={conversation_id}, æ¨¡å¼={config.MEMORY_SAVE_MODE}, æ¶ˆæ¯æ•°é‡={len(messages_to_save)}, roles={roles}")
                
                # è®°å½•å°†è¦ä¿å­˜çš„æ¶ˆæ¯å†…å®¹ï¼ˆä¸ºäº†é¿å…æ—¥å¿—è¿‡å¤§ï¼Œå¯ä»¥åªè®°å½•ç¬¬ä¸€æ¡å’Œæœ€åä¸€æ¡ï¼‰
                if len(messages_to_save) > 0:
                    log.debug(f"ğŸ’¾ ç¬¬ä¸€æ¡æ¶ˆæ¯é¢„è§ˆ: conversation_id={conversation_id}, role={messages_to_save[0].get('role')}, content='{messages_to_save[0].get('content', '')[:100]}...'")
                    if len(messages_to_save) > 1:
                        log.debug(f"ğŸ’¾ æœ€åä¸€æ¡æ¶ˆæ¯é¢„è§ˆ: conversation_id={conversation_id}, role={messages_to_save[-1].get('role')}, content='{messages_to_save[-1].get('content', '')[:100]}...'")
                
                # æ‰¹é‡ä¿å­˜æ¶ˆæ¯
                await self.async_chat_memory.add_messages_batch(conversation_id, messages_to_save)
                log.debug(f"âœ… æ¶ˆæ¯ä¿å­˜åˆ°è®°å¿†æˆåŠŸ: conversation_id={conversation_id}, ä¿å­˜äº†{len(messages_to_save)}æ¡æ¶ˆæ¯")
            else:
                log.debug(f"âš ï¸ æ ¹æ®é…ç½® MEMORY_SAVE_MODE={config.MEMORY_SAVE_MODE}ï¼Œæ²¡æœ‰æ¶ˆæ¯éœ€è¦ä¿å­˜åˆ°è®°å¿†: conversation_id={conversation_id}")
        except Exception as e:
            log.error(f"âŒ ä¿å­˜æ¶ˆæ¯åˆ°è®°å¿†å¤±è´¥: conversation_id={conversation_id}, error={e}", exc_info=True)

    # ä¿ç•™åŸæœ‰çš„_save_message_to_memory_asyncæ–¹æ³•ä»¥ä¿æŒå‘åå…¼å®¹
    async def _save_message_to_memory_async(self, conversation_id: str, message: dict):
        """å¼‚æ­¥ä¿å­˜æ¶ˆæ¯åˆ°è®°å¿†ï¼Œä¸é˜»å¡ä¸»çº¿ç¨‹ - ä¸ºä¿æŒå‘åå…¼å®¹æ€§ä¿ç•™"""
        try:
            # ç›´æ¥è°ƒç”¨æ–°çš„å¼‚æ­¥æ–¹æ³•
            await self._async_save_message_to_memory(conversation_id, [message])
        except Exception as e:
            log.error(f"å¼‚æ­¥ä¿å­˜æ¶ˆæ¯åˆ°è®°å¿†å¤±è´¥: {e}")
    
    # ========== BaseEngineæ¥å£å®ç° ==========
    
    async def get_engine_info(self) -> Dict[str, Any]:
        """è·å–å¼•æ“ä¿¡æ¯"""
        return {
            "name": "chat_engine",
            "version": "2.0.0",
            "features": [
                EngineCapabilities.MEMORY,
                EngineCapabilities.TOOLS,
                EngineCapabilities.PERSONALITY,
                EngineCapabilities.STREAMING,
                EngineCapabilities.PERFORMANCE_MONITORING,
                EngineCapabilities.CACHE,
                EngineCapabilities.MCP_INTEGRATION
            ],
            "status": EngineStatus.HEALTHY,
            "description": "ä¸»èŠå¤©å¼•æ“ï¼Œé›†æˆMemoryç¼“å­˜ã€æ€§èƒ½ç›‘æ§ã€å·¥å…·è°ƒç”¨å’ŒMCP"
        }
    
    async def health_check(self) -> Dict[str, Any]:
        """å¥åº·æ£€æŸ¥"""
        timestamp = time.time()
        errors = []
        
        # æ£€æŸ¥OpenAI API
        openai_healthy = True
        try:
            # ç®€å•æµ‹è¯•ï¼Œä¸å®é™…è°ƒç”¨API
            if not self.sync_client or not config.OPENAI_API_KEY:
                openai_healthy = False
                errors.append("OpenAI APIé…ç½®ä¸å®Œæ•´")
        except Exception as e:
            openai_healthy = False
            errors.append(f"OpenAI APIæ£€æŸ¥å¤±è´¥: {str(e)}")
        
        # æ£€æŸ¥Memoryç³»ç»Ÿ
        memory_healthy = True
        try:
            if not self.async_chat_memory:
                memory_healthy = False
                errors.append("Memoryç³»ç»Ÿæœªåˆå§‹åŒ–")
        except Exception as e:
            memory_healthy = False
            errors.append(f"Memoryç³»ç»Ÿæ£€æŸ¥å¤±è´¥: {str(e)}")
        
        # æ£€æŸ¥å·¥å…·ç³»ç»Ÿ
        tool_healthy = True
        try:
            tool_count = len(tool_registry.list_tools())
            if tool_count == 0:
                log.warning("å·¥å…·ç³»ç»Ÿæ— å¯ç”¨å·¥å…·")
        except Exception as e:
            tool_healthy = False
            errors.append(f"å·¥å…·ç³»ç»Ÿæ£€æŸ¥å¤±è´¥: {str(e)}")
        
        # æ£€æŸ¥äººæ ¼ç³»ç»Ÿ
        personality_healthy = True
        try:
            personalities = self.personality_manager.get_all_personalities()
            if not personalities:
                log.warning("äººæ ¼ç³»ç»Ÿæ— å¯ç”¨äººæ ¼")
        except Exception as e:
            personality_healthy = False
            errors.append(f"äººæ ¼ç³»ç»Ÿæ£€æŸ¥å¤±è´¥: {str(e)}")
        
        # ç»¼åˆåˆ¤æ–­
        all_healthy = openai_healthy and memory_healthy and tool_healthy and personality_healthy
        
        return {
            "healthy": all_healthy,
            "timestamp": timestamp,
            "details": {
                "openai_api": openai_healthy,
                "memory_system": memory_healthy,
                "tool_system": tool_healthy,
                "personality_system": personality_healthy
            },
            "errors": errors
        }
    
    async def clear_conversation_memory(self, conversation_id: str) -> Dict[str, Any]:
        """æ¸…é™¤æŒ‡å®šä¼šè¯çš„è®°å¿†"""
        try:
            # è·å–å½“å‰è®°å¿†æ•°é‡
            current_memories = await self.async_chat_memory.get_all_memory(conversation_id)
            count_before = len(current_memories) if current_memories else 0
            
            # æ¸…é™¤è®°å¿†
            await self.async_chat_memory.delete_memory(conversation_id)
            
            return {
                "success": True,
                "conversation_id": conversation_id,
                "deleted_count": count_before,
                "message": f"å·²æ¸…é™¤ä¼šè¯ {conversation_id} çš„ {count_before} æ¡è®°å¿†"
            }
        except Exception as e:
            log.error(f"æ¸…é™¤ä¼šè¯è®°å¿†å¤±è´¥: {e}")
            return {
                "success": False,
                "conversation_id": conversation_id,
                "deleted_count": 0,
                "message": f"æ¸…é™¤å¤±è´¥: {str(e)}"
            }
    
    async def get_conversation_memory(
        self,
        conversation_id: str,
        limit: Optional[int] = None
    ) -> Dict[str, Any]:
        """è·å–æŒ‡å®šä¼šè¯çš„è®°å¿†"""
        try:
            # è·å–æ‰€æœ‰è®°å¿†
            all_memories = await self.async_chat_memory.get_all_memory(conversation_id)
            
            if not all_memories:
                return {
                    "success": True,
                    "conversation_id": conversation_id,
                    "memories": [],
                    "total_count": 0,
                    "returned_count": 0
                }
            
            total_count = len(all_memories)
            
            # åº”ç”¨limit
            if limit and limit > 0:
                memories = all_memories[:limit]
            else:
                memories = all_memories
            
            return {
                "success": True,
                "conversation_id": conversation_id,
                "memories": memories,
                "total_count": total_count,
                "returned_count": len(memories)
            }
        except Exception as e:
            log.error(f"è·å–ä¼šè¯è®°å¿†å¤±è´¥: {e}")
            return {
                "success": False,
                "conversation_id": conversation_id,
                "memories": [],
                "total_count": 0,
                "returned_count": 0,
                "error": str(e)
            }
    
    async def get_supported_personalities(self) -> List[Dict[str, Any]]:
        """è·å–æ”¯æŒçš„äººæ ¼åˆ—è¡¨"""
        try:
            personalities = self.personality_manager.get_all_personalities()
            result = []
            
            for pid, pdata in personalities.items():
                result.append({
                    "id": pid,
                    "name": pdata.get("name", pid),
                    "description": pdata.get("system_prompt", "")[:100] + "...",  # æˆªå–å‰100å­—ç¬¦
                    "allowed_tools": [tool.get("tool_name") for tool in pdata.get("allowed_tools", [])]
                })
            
            return result
        except Exception as e:
            log.error(f"è·å–äººæ ¼åˆ—è¡¨å¤±è´¥: {e}")
            return []
    
    async def get_available_tools(self, personality_id: Optional[str] = None) -> List[Dict[str, Any]]:
        """è·å–å¯ç”¨çš„å·¥å…·åˆ—è¡¨"""
        try:
            # è·å–æ‰€æœ‰å·¥å…·
            all_tools = tool_registry.list_tools()
            
            # å¦‚æœæŒ‡å®šäº†äººæ ¼ï¼Œè¿‡æ»¤å·¥å…·
            if personality_id:
                personality = self.personality_manager.get_personality(personality_id)
                if personality and personality.get("allowed_tools"):
                    allowed_tool_names = [tool["tool_name"] for tool in personality["allowed_tools"]]
                    # è¿‡æ»¤å·¥å…·
                    all_tools = {name: tool for name, tool in all_tools.items() if name in allowed_tool_names}
            
            # è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼
            result = []
            for name, tool_class in all_tools.items():
                tool_instance = tool_class()
                result.append({
                    "name": tool_instance.name,
                    "description": tool_instance.description,
                    "parameters": tool_instance.parameters
                })
            
            return result
        except Exception as e:
            log.error(f"è·å–å·¥å…·åˆ—è¡¨å¤±è´¥: {e}")
            return []
    
    async def get_allowed_tools_schema(self, personality_id: Optional[str] = None) -> List[Dict]:
        """æ ¹æ®personalityè·å–å…è®¸çš„å·¥å…·ï¼ˆOpenAI schemaæ ¼å¼ï¼‰
        
        Args:
            personality_id: äººæ ¼IDï¼Œå¦‚æœä¸ºNoneåˆ™è¿”å›æ‰€æœ‰å·¥å…·
            
        Returns:
            List[Dict]: OpenAIå‡½æ•°schemaæ ¼å¼çš„å·¥å…·åˆ—è¡¨
        """
        try:
            # è·å–æ‰€æœ‰å·¥å…·çš„OpenAIå‡½æ•°schema
            all_tools_schema = tool_registry.get_functions_schema()
            
            # å¦‚æœæ²¡æœ‰æŒ‡å®špersonalityï¼Œè¿”å›æ‰€æœ‰å·¥å…·
            if not personality_id:
                log.info(f"æœªæŒ‡å®špersonalityï¼Œè¿”å›æ‰€æœ‰å·¥å…·ï¼Œå…± {len(all_tools_schema)} ä¸ª")
                return all_tools_schema
            
            # è·å–personalityé…ç½®
            personality = self.personality_manager.get_personality(personality_id)
            if not personality or not personality.allowed_tools:
                # å¦‚æœpersonalityæ²¡æœ‰å·¥å…·é™åˆ¶ï¼Œè¿”å›æ‰€æœ‰å·¥å…·
                log.info(f"personality {personality_id} æ²¡æœ‰å·¥å…·é™åˆ¶ï¼Œè¿”å›æ‰€æœ‰å·¥å…·ï¼Œå…± {len(all_tools_schema)} ä¸ª")
                return all_tools_schema
            
            # æå–å…è®¸çš„å·¥å…·åç§°ï¼ˆå…¼å®¹tool_nameå’Œnameä¸¤ç§å­—æ®µï¼‰
            allowed_tool_names = []
            for tool in personality.allowed_tools:
                if 'tool_name' in tool:
                    allowed_tool_names.append(tool['tool_name'])
                elif 'name' in tool:
                    allowed_tool_names.append(tool['name'])
            
            if not allowed_tool_names:
                log.warning(f"personality {personality_id} çš„allowed_toolsæ ¼å¼ä¸æ­£ç¡®ï¼Œä½¿ç”¨æ‰€æœ‰å¯ç”¨å·¥å…·")
                return all_tools_schema
            
            # æ ¹æ®allowed_toolsè¿‡æ»¤å·¥å…·schema
            filtered_tools = [
                tool for tool in all_tools_schema 
                if tool.get('function', {}).get('name') in allowed_tool_names
            ]
            # å…œåº•ï¼šè‹¥å…è®¸åˆ—è¡¨ä¸­çš„æŸäº›å·¥å…·æœªå‡ºç°åœ¨schemaé‡Œï¼ˆå¸¸è§äºMCPå·¥å…·æœªåŠæ—¶æ³¨å†Œï¼‰
            if len(filtered_tools) < len(allowed_tool_names):
                from services.mcp.discovery import discover_and_register_mcp_tools
                missing = [name for name in allowed_tool_names if name not in {t.get('function', {}).get('name') for t in filtered_tools}]
                if missing:
                    log.info(f"å…è®¸å·¥å…·ä¸­ç¼ºå°‘schemaï¼Œå°è¯•å‘ç°å¹¶æ³¨å†ŒMCPå·¥å…·: {missing}")
                    try:
                        discover_and_register_mcp_tools()
                        # é‡æ–°è·å–å¹¶è¿‡æ»¤
                        all_tools_schema = tool_registry.get_functions_schema()
                        filtered_tools = [
                            tool for tool in all_tools_schema 
                            if tool.get('function', {}).get('name') in allowed_tool_names
                        ]
                        log.info(f"MCPå·¥å…·æ³¨å†Œåå¯ç”¨schemaæ•°é‡: {len(filtered_tools)}")
                    except Exception as e:
                        log.warning(f"MCPå·¥å…·å‘ç°å¤±è´¥ï¼Œç»§ç»­ä½¿ç”¨ç°æœ‰schema: {e}")
            
            log.info(f"åº”ç”¨personality {personality_id} çš„å·¥å…·é™åˆ¶ï¼Œå…è®¸çš„å·¥å…·: {allowed_tool_names}, è¿‡æ»¤åæ•°é‡: {len(filtered_tools)}/{len(all_tools_schema)}")
            
            return filtered_tools
            
        except Exception as e:
            log.error(f"è·å–å…è®¸çš„å·¥å…·schemaå¤±è´¥: {e}")
            # å‡ºé”™æ—¶è¿”å›ç©ºåˆ—è¡¨ï¼Œé¿å…æš´éœ²æ‰€æœ‰å·¥å…·
            return []


# åˆ›å»ºå…¨å±€èŠå¤©å¼•æ“å®ä¾‹
chat_engine = ChatEngine()
